# ğŸ“„ RAG Document Assistant (Local, Streaming, Scalable)

A **production-ready Retrieval-Augmented Generation (RAG) system** that allows users to upload documents (PDF / CSV / Excel) and ask questions with **live streaming AI responses**, running **entirely locally** using Ollama.

This project demonstrates **real-world AI backend engineering**, not a toy example.

---

## ğŸš€ Features

- ğŸ“¤ Upload documents (PDF, CSV, Excel)
- ğŸ§  Automatic document parsing & chunking
- ğŸ”¢ Vector embeddings using PostgreSQL + pgvector
- ğŸ” Semantic search over document content
- ğŸ’¬ Streaming answers (token-by-token)
- ğŸ§‘ Multi-user safe (user-scoped data)
- ğŸ  Fully local setup (no OpenAI required)
- âš¡ FastAPI + async PostgreSQL backend
- ğŸ¨ Streamlit frontend UI

---

## ğŸ§± Architecture Overview

User (Streamlit UI)
    â†“
Upload Document
    â†“
FastAPI Backend
    â”œâ”€ Parse document
    â”œâ”€ Chunk text
    â”œâ”€ Generate embeddings (Ollama)
    â”œâ”€ Store in PostgreSQL (pgvector)
    â†“
Ask Question
    â†“
Vector similarity search
    â†“
Prompt construction
    â†“
Streaming LLM answer (Ollama)

---

## ğŸ› ï¸ Tech Stack

### Backend
- FastAPI
- PostgreSQL + pgvector
- asyncpg

### AI / NLP
- Ollama
  - qwen2:1.5b (LLM)
  - nomic-embed-text (Embeddings)

### Frontend
- Streamlit
- Live streaming UI

---

## âš™ï¸ Setup Instructions

### PostgreSQL + pgvector
Install PostgreSQL and pgvector, then enable the extension:

CREATE EXTENSION vector;

### Ollama
Install and pull models:

ollama pull qwen2:1.5b
ollama pull nomic-embed-text

### Backend
Run:
uvicorn app.main:app --reload

### Frontend
Run:
streamlit run streamlit.py

---

## ğŸ‘¨â€ğŸ’» Author

Shubham Singh